# Meta-Abilities Alignment – Data Generators

This repository contains two generators—one for the **Deduction** track and one for the **Induction** track—described in  
*“Beyond ‘Aha!’ Toward Systematic Meta-Abilities Alignment in Large Reasoning Models.”* :contentReference[oaicite:1]{index=1}

```
# Generate Deduction datasets (Propositional Satisfiability)
python Deduction.py

# Generate Induction datasets (Masked-Sequence Completion)
python Induction.py

# Generate Abduction datasets (Reverse Rule-Graph Search)
python Abduction.py
```


---

## 1  Deduction – Propositional Satisfiability

The generator produces diagnostic **Deduction** task, i.e. small instances of *propositional satisfiability* over **nested Boolean formulas**.  
Each puzzle is parameterised by the triple  
\[
\langle n_\ell,\; f_\ell,\; d_\ell\rangle
= (\text{number of variables},\; \text{number of formulas},\; \text{max nesting depth})
\]

| Curriculum Level | \( n_\ell \) (`n_vars`) | \( d_\ell \) (`max_depth`) | typical \( f_\ell \) |
|------------------|-------------------------|---------------------------|----------------------|
| 1 | 4 | 1 | 3 – 4 |
| 2 | 6 | 2 | 3 – 5 |
| 3 | 8 | 3 | 4 – 5 |
| 4 | 10 | 4 | 4 – 6 |
| 5 | 12 | 5 | 4 – 6 |

* **Search complexity** grows as \(2^{n_\ell}\). Deeper nesting couples distant variables via → and ↔, so heuristic pruning is ineffective; moving from \((6,2,3)\) to \((10,4,6)\) increases naïve back-tracking time by roughly two orders of magnitude. :contentReference[oaicite:7]{index=7}

* A puzzle is solved when **one** assignment satisfies **all** formulas (or the instance is declared UNSAT). This matches the evaluator used for training rewards: a deduction answer counts as correct *only* if every clause evaluates to `True`. :contentReference[oaicite:8]{index=8}

Each JSONL entry has  
```jsonc
{
  "index": 123,
  "puzzle_text": "Below are some nested formulas:\n  - (¬D ∨ B)\n  - ...\nPlease list the truth value of each variable\n",
  "solution_text": "(1) A is True\n(2) B is False\n...",
}


## 2 Induction – Masked-Sequence Completion

A puzzle presents a **numeric sequence** generated by a hidden *k*-step cyclic rule and asks for the value at the final “?” placeholder.

### 2.1 Generation parameters

⟨k, L, 𝒪⟩ = (cycle length, visible length, operation multiset)

- **Cycle length k.** Curriculum levels map to k = 6 … 10.
- **Visible length L.** Uniform in 7, 11.
- **Operation multiset 𝒪.** Each step independently sampled from `+a`, `−b`, or `×c` with small integers a, b ∈ [1, 4], c ∈ [2, 4].

The rule repeats every k positions:

\[
x_{t+k} = \mathcal{O}_k(\dots \mathcal{O}_2(\mathcal{O}_1(x_t))).
\]

### 2.2 Curriculum table & files

| Level | k  | JSON file |
|:-----:|:--:|:---------:|
| 1     | 6  | `1.json` |
| 2     | 7  | `2.json` |
| 3     | 8  | `3.json` |
| 4     | 9  | `4.json` |
| 5     | 10 | `5.json` |

### 2.3 Task definition & difficulty

The effective branching factor is ≈ 9 (three op types × three mean magnitudes).  
Thus brute-force search grows as 9ᵏ and becomes intractable beyond k ≈ 8, forcing models to generalise the latent rule rather than enumerate.

**Scoring rule.** Exact numeric match with ground-truth target.

```jsonc
{
  // example record (3.jsonl)
  "problem_id": "421",
  "difficulty": 3,
  "puzzle_text": "Given the following sequence,\n[2, 4, 3, 6, 5, 10, 9, ?]\nWhat is the value of the final “?” placeholder?",
  "solution_text": 18,
  "complete_sequence": [2, 4, 3, 6, 5, 10, 9, 18]
}

## 3 Abduction – Reverse Rule-Graph Search

A puzzle supplies a set of **Horn-style implications** (premises) and a list of **goals**; the task is to construct a minimal backward proof (reverse rule-graph) that explains which goals are reached from known facts and to distinguish reachable vs. unreachable goals.

### 3.1 Generation parameters

⟨dₗ, gₗ, hₗ, γ⟩ = (chain depth, #goals, #distractors, cycle probability)

- **Chain depth (dₗ).** Maximum length of backward inference chains.  
- **# of goals (gₗ).** Total sink nodes the agent must explain.  
- **# of distractors (hₗ).** Extra random premises sharing symbols.  
- **Cycle probability (γ).** Likelihood of introducing cyclic dependencies.

Per-level ranges (Table 3, supplementary):

| Level | dₗ       | gₗ | hₗ      | γ    |
|:-----:|:--------:|:--:|:-------:|:----:|
| 1     | 2–3      | 1  | 3–5     | 0.10 |
| 2     | 3–4      | 2  | 5–7     | 0.15 |
| 3     | 4–5      | 2  | 7–9     | 0.20 |
| 4     | 5–6      | 3  | 8–10    | 0.25 |
| 5     | 6–7      | 3  | 10–12   | 0.30 |


### 3.2 Task definition & difficulty

- **Consistency check.** Brute-force enumeration over all valuations (2ᴺ with N ≤20) to ensure premises are satisfiable and goals correctly classified.  
- **Challenge scaling.** Increasing chain depth or distractors dramatically expands the reverse proof search space (≈ branching_factorᵈₗ).  
- **Scoring.** Puzzle is valid only if known facts hold and reachable/unreachable goals match ground truth; invalid puzzles are discarded.

```jsonc
{
  "problem_id": "42",
  "premises": ["(A AND B) => C", "(C) => D", ...],
  "known_atoms": ["A", "B"],
  "goals": ["D", "E"],
  "reachable_goals": ["D"],
  "unreachable_goals": ["E"]
}


