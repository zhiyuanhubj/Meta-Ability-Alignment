# Meta-Abilities Alignment â€“ Data Generators

This repository contains two generatorsâ€”one for the **Deduction** track and one for the **Induction** trackâ€”described in  
*â€œBeyond â€˜Aha!â€™ Toward Systematic Meta-Abilities Alignment in Large Reasoning Models.â€* :contentReference[oaicite:1]{index=1}

```
# Generate Deduction datasets (Propositional Satisfiability)
python Deduction.py

# Generate Induction datasets (Masked-Sequence Completion)
python Induction.py

# Generate Abduction datasets (Reverse Rule-Graph Search)
python Abduction.py
```


---

## 1  Deduction â€“ Propositional Satisfiability

The generator produces diagnostic **Deduction** task, i.e. small instances of *propositional satisfiability* over **nested Boolean formulas**.  
Each puzzle is parameterised by the triple  
\[
\langle n_\ell,\; f_\ell,\; d_\ell\rangle
= (\text{number of variables},\; \text{number of formulas},\; \text{max nesting depth})
\]

| Curriculum Level | \( n_\ell \) (`n_vars`) | \( d_\ell \) (`max_depth`) | typical \( f_\ell \) |
|------------------|-------------------------|---------------------------|----------------------|
| 1 | 4 | 1 | 3 â€“ 4 |
| 2 | 6 | 2 | 3 â€“ 5 |
| 3 | 8 | 3 | 4 â€“ 5 |
| 4 | 10 | 4 | 4 â€“ 6 |
| 5 | 12 | 5 | 4 â€“ 6 |

* **Search complexity** grows as \(2^{n_\ell}\). Deeper nesting couples distant variables via â†’ and â†”, so heuristic pruning is ineffective; moving from \((6,2,3)\) to \((10,4,6)\) increases naÃ¯ve back-tracking time by roughly two orders of magnitude. :contentReference[oaicite:7]{index=7}

* A puzzle is solved when **one** assignment satisfies **all** formulas (or the instance is declared UNSAT). This matches the evaluator used for training rewards: a deduction answer counts as correct *only* if every clause evaluates to `True`. :contentReference[oaicite:8]{index=8}

Each JSONL entry has  
```jsonc
{
  "index": 123,
  "puzzle_text": "Below are some nested formulas:\n  - (Â¬D âˆ¨ B)\n  - ...\nPlease list the truth value of each variable\n",
  "solution_text": "(1) A is True\n(2) B is False\n...",
}


## 2 Induction â€“ Masked-Sequence Completion

A puzzle presents a **numeric sequence** generated by a hidden *k*-step cyclic rule and asks for the value at the final â€œ?â€ placeholder.

### 2.1 Generation parameters

âŸ¨k, L, ð’ªâŸ© = (cycle length, visible length, operation multiset)

- **Cycle length k.** Curriculum levels map to k = 6 â€¦ 10.
- **Visible length L.** Uniform in 7, 11.
- **Operation multiset ð’ª.** Each step independently sampled from `+a`, `âˆ’b`, or `Ã—c` with small integers a, b âˆˆ [1, 4], c âˆˆ [2, 4].

The rule repeats every k positions:

\[
x_{t+k} = \mathcal{O}_k(\dots \mathcal{O}_2(\mathcal{O}_1(x_t))).
\]

### 2.2 Curriculum table & files

| Level | k  | JSON file |
|:-----:|:--:|:---------:|
| 1     | 6  | `1.json` |
| 2     | 7  | `2.json` |
| 3     | 8  | `3.json` |
| 4     | 9  | `4.json` |
| 5     | 10 | `5.json` |

### 2.3 Task definition & difficulty

The effective branching factor is â‰ˆ 9 (three op types Ã— three mean magnitudes).  
Thus brute-force search grows as 9áµ and becomes intractable beyond k â‰ˆ 8, forcing models to generalise the latent rule rather than enumerate.

**Scoring rule.** Exact numeric match with ground-truth target.

```jsonc
{
  // example record (3.jsonl)
  "problem_id": "421",
  "difficulty": 3,
  "puzzle_text": "Given the following sequence,\n[2, 4, 3, 6, 5, 10, 9, ?]\nWhat is the value of the final â€œ?â€ placeholder?",
  "solution_text": 18,
  "complete_sequence": [2, 4, 3, 6, 5, 10, 9, 18]
}

## 3 Abduction â€“ Reverse Rule-Graph Search

A puzzle supplies a set of **Horn-style implications** (premises) and a list of **goals**; the task is to construct a minimal backward proof (reverse rule-graph) that explains which goals are reached from known facts and to distinguish reachable vs. unreachable goals.

### 3.1 Generation parameters

âŸ¨dâ‚—, gâ‚—, hâ‚—, Î³âŸ© = (chain depth, #goals, #distractors, cycle probability)

- **Chain depth (dâ‚—).** Maximum length of backward inference chains.  
- **# of goals (gâ‚—).** Total sink nodes the agent must explain.  
- **# of distractors (hâ‚—).** Extra random premises sharing symbols.  
- **Cycle probability (Î³).** Likelihood of introducing cyclic dependencies.

Per-level ranges (Table 3, supplementary):

| Level | dâ‚—       | gâ‚— | hâ‚—      | Î³    |
|:-----:|:--------:|:--:|:-------:|:----:|
| 1     | 2â€“3      | 1  | 3â€“5     | 0.10 |
| 2     | 3â€“4      | 2  | 5â€“7     | 0.15 |
| 3     | 4â€“5      | 2  | 7â€“9     | 0.20 |
| 4     | 5â€“6      | 3  | 8â€“10    | 0.25 |
| 5     | 6â€“7      | 3  | 10â€“12   | 0.30 |


### 3.2 Task definition & difficulty

- **Consistency check.** Brute-force enumeration over all valuations (2á´º with N â‰¤20) to ensure premises are satisfiable and goals correctly classified.  
- **Challenge scaling.** Increasing chain depth or distractors dramatically expands the reverse proof search space (â‰ˆ branching_factoráµˆâ‚—).  
- **Scoring.** Puzzle is valid only if known facts hold and reachable/unreachable goals match ground truth; invalid puzzles are discarded.

```jsonc
{
  "problem_id": "42",
  "premises": ["(A AND B) => C", "(C) => D", ...],
  "known_atoms": ["A", "B"],
  "goals": ["D", "E"],
  "reachable_goals": ["D"],
  "unreachable_goals": ["E"]
}


